{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cn68sXm60ve"
      },
      "outputs": [],
      "source": [
        "https://github.com/sizzcode/build_test.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Xae421z7XcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q chromadb sentence-transformers GitPython langchain huggingface_hub\n",
        "!pip install -q --upgrade google-generativeai\n",
        "\n",
        "import os\n",
        "import chromadb\n",
        "import tempfile\n",
        "import git\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Step 1: Define helper functions for code parsing and storage\n",
        "\n",
        "def clone_github_repo(repo_url: str, local_path: str) -> str:\n",
        "    \"\"\"Clone a GitHub repository to a local path.\"\"\"\n",
        "    print(f\"Cloning repository {repo_url} to {local_path}...\")\n",
        "    try:\n",
        "        git.Repo.clone_from(repo_url, local_path)\n",
        "        print(\"Repository cloned successfully.\")\n",
        "        return local_path\n",
        "    except git.GitCommandError as e:\n",
        "        print(f\"Error cloning repository: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_file_contents(repo_path: str, file_extensions: List[str] = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Extract content from files in the repository, optionally filtering by extension.\n",
        "    Returns a list of dictionaries with file paths and contents.\n",
        "    \"\"\"\n",
        "    if file_extensions is None:\n",
        "        # Default to common code file extensions\n",
        "        file_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.h', '.hpp', '.cs', '.go', '.rb', '.php', '.ts', '.html', '.css']\n",
        "\n",
        "    all_files = []\n",
        "    # Walk through the repository\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        # Skip hidden folders like .git\n",
        "        if any(part.startswith('.') for part in Path(root).parts):\n",
        "            continue\n",
        "\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            # Check if the file has one of the desired extensions\n",
        "            if any(file.endswith(ext) for ext in file_extensions):\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "\n",
        "                    # Calculate relative path from repo_path\n",
        "                    rel_path = os.path.relpath(file_path, repo_path)\n",
        "                    all_files.append({\n",
        "                        \"path\": rel_path,\n",
        "                        \"content\": content\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "    print(f\"Extracted content from {len(all_files)} files.\")\n",
        "    return all_files\n",
        "\n",
        "def split_code_into_chunks(file_contents: List[Dict[str, Any]], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Split code files into smaller chunks for better vector storage and retrieval.\n",
        "    Each chunk keeps reference to its original file.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "    )\n",
        "\n",
        "    chunks = []\n",
        "    for file_info in file_contents:\n",
        "        file_path = file_info[\"path\"]\n",
        "        content = file_info[\"content\"]\n",
        "\n",
        "        # Split the content into chunks\n",
        "        content_chunks = text_splitter.split_text(content)\n",
        "\n",
        "        # Create a document for each chunk\n",
        "        for i, chunk in enumerate(content_chunks):\n",
        "            chunks.append({\n",
        "                \"path\": file_path,\n",
        "                \"chunk_id\": f\"{file_path}_chunk_{i}\",\n",
        "                \"content\": chunk,\n",
        "                \"metadata\": {\n",
        "                    \"path\": file_path,\n",
        "                    \"chunk_number\": i,\n",
        "                    \"total_chunks\": len(content_chunks)\n",
        "                }\n",
        "            })\n",
        "\n",
        "    print(f\"Split {len(file_contents)} files into {len(chunks)} chunks.\")\n",
        "    return chunks\n",
        "\n",
        "# Step 2: Set up embedding model and vector storage\n",
        "\n",
        "class CodeEmbeddingSystem:\n",
        "    def __init__(self, persist_directory: str = \"./chroma_db\"):\n",
        "        \"\"\"Initialize the embedding system with HuggingFace and ChromaDB.\"\"\"\n",
        "        self.persist_directory = persist_directory\n",
        "\n",
        "        # Initialize the embedding model (free alternative to OpenAI)\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "        # Initialize ChromaDB (free alternative to Pinecone)\n",
        "        print(\"Initializing ChromaDB...\")\n",
        "        self.chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
        "\n",
        "        # Create or get the collection\n",
        "        self.collection = self.chroma_client.get_or_create_collection(\n",
        "            name=\"code_repository\",\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "        print(\"Embedding system initialized.\")\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of text strings.\"\"\"\n",
        "        embeddings = self.embedding_model.encode(texts)\n",
        "        return embeddings.tolist()\n",
        "\n",
        "    def add_chunks_to_db(self, chunks: List[Dict[str, Any]]) -> None:\n",
        "        \"\"\"Add code chunks to the vector database.\"\"\"\n",
        "        if not chunks:\n",
        "            print(\"No chunks to add to the database.\")\n",
        "            return\n",
        "\n",
        "        # Prepare data for ChromaDB\n",
        "        ids = [chunk[\"chunk_id\"] for chunk in chunks]\n",
        "        documents = [chunk[\"content\"] for chunk in chunks]\n",
        "        metadatas = [chunk[\"metadata\"] for chunk in chunks]\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = self.generate_embeddings(documents)\n",
        "\n",
        "        # Add to ChromaDB\n",
        "        self.collection.add(\n",
        "            ids=ids,\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            embeddings=embeddings\n",
        "        )\n",
        "\n",
        "        print(f\"Added {len(chunks)} chunks to the vector database.\")\n",
        "\n",
        "    def query_similar_code(self, error_message: str, n_results: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Query the vector database for code similar to the error message.\"\"\"\n",
        "        # Generate embedding for the error message\n",
        "        error_embedding = self.generate_embeddings([error_message])[0]\n",
        "\n",
        "        # Query ChromaDB\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[error_embedding],\n",
        "            n_results=n_results,\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "        )\n",
        "\n",
        "        # Format results\n",
        "        formatted_results = []\n",
        "        for i in range(len(results[\"ids\"][0])):\n",
        "            formatted_results.append({\n",
        "                \"id\": results[\"ids\"][0][i],\n",
        "                \"content\": results[\"documents\"][0][i],\n",
        "                \"metadata\": results[\"metadatas\"][0][i],\n",
        "                \"similarity\": 1 - results[\"distances\"][0][i]  # Convert distance to similarity\n",
        "            })\n",
        "\n",
        "        return formatted_results\n",
        "\n",
        "# Step 3: LLM integration for error analysis and fix generation using Gemini API\n",
        "\n",
        "class ErrorAnalysisSystem:\n",
        "    def __init__(self, api_key: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the error analysis system with Google's Gemini API.\n",
        "\n",
        "        Args:\n",
        "            api_key (str): Your Gemini API key. Required for generating fixes.\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        if not api_key:\n",
        "            print(\"WARNING: No Gemini API key provided. You will need to provide an API key before generating fixes.\")\n",
        "\n",
        "    def setup_gemini_in_colab(self):\n",
        "        \"\"\"Set up Gemini in Google Colab.\"\"\"\n",
        "        print(\"Setting up Gemini API access in Google Colab...\")\n",
        "\n",
        "        # Install the Google Generative AI Python SDK\n",
        "        !pip install -q google-generativeai\n",
        "\n",
        "        print(\"Gemini SDK installed. Make sure to set your API key using:\")\n",
        "        print(\"resolver.error_analyzer.api_key = 'YOUR_GEMINI_API_KEY'\")\n",
        "\n",
        "    def generate_fix(self, error_message: str, code_contexts: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Generate a fix for the error using the Gemini API.\"\"\"\n",
        "        if not self.api_key:\n",
        "            return \"ERROR: No Gemini API key provided. Please set the API key before generating fixes.\"\n",
        "\n",
        "        try:\n",
        "            import google.generativeai as genai\n",
        "\n",
        "            # Configure the Gemini API with your key\n",
        "            genai.configure(api_key=self.api_key)\n",
        "\n",
        "            # Prepare the context with error and code\n",
        "            context_text = \"\\n\\n\".join([\n",
        "                f\"File: {context['metadata']['path']}\\n```\\n{context['content']}\\n```\"\n",
        "                for context in code_contexts[:3]  # Limit to top 3 results for brevity\n",
        "            ])\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            You are an expert programmer. Given the following error message and relevant code from a repository,\n",
        "            identify the likely cause of the error and suggest a fix.\n",
        "\n",
        "            ERROR MESSAGE:\n",
        "            {error_message}\n",
        "\n",
        "            RELEVANT CODE CONTEXT:\n",
        "            {context_text}\n",
        "\n",
        "            Please analyze the error and provide:\n",
        "            1. The root cause of the error\n",
        "            2. A specific fix for the error\n",
        "            3. Any additional context or explanation about why this fix works\n",
        "\n",
        "            Your solution should be clear, concise, and directly address the error.\n",
        "            \"\"\"\n",
        "\n",
        "            # List available models to debug\n",
        "            print(\"Available Gemini models:\")\n",
        "            for m in genai.list_models():\n",
        "                if \"gemini\" in m.name.lower():\n",
        "                    print(f\" - {m.name}\")\n",
        "\n",
        "            # Get the correct model\n",
        "            # The model name format may be different depending on the SDK version\n",
        "            # Most recent versions use \"models/gemini-1.0-pro\" or similar\n",
        "            try:\n",
        "                model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "            except:\n",
        "                try:\n",
        "                    model = genai.GenerativeModel('models/gemini-pro')\n",
        "                except:\n",
        "                    # Final fallback\n",
        "                    available_models = [m.name for m in genai.list_models() if \"gemini\" in m.name.lower()]\n",
        "                    if available_models:\n",
        "                        model = genai.GenerativeModel(available_models[0])\n",
        "                    else:\n",
        "                        return \"ERROR: No Gemini models available with your API key.\"\n",
        "\n",
        "            # Generate the response\n",
        "            print(\"Querying Gemini API for error analysis and fix...\")\n",
        "            response = model.generate_content(prompt)\n",
        "\n",
        "            # Process and return the response\n",
        "            if hasattr(response, 'text'):\n",
        "                return response.text\n",
        "            elif hasattr(response, 'parts'):\n",
        "                return \"\\n\".join([part.text for part in response.parts])\n",
        "            else:\n",
        "                # Handle different response formats (may vary based on API version)\n",
        "                return str(response)\n",
        "\n",
        "        except ImportError:\n",
        "            return \"ERROR: Google GenerativeAI package not installed. Run 'pip install google-generativeai' first.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating fix: {e}\")\n",
        "            error_details = f\"Error generating fix: {str(e)}\\n\\n\"\n",
        "            error_details += \"Try these troubleshooting steps:\\n\"\n",
        "            error_details += \"1. Check that your API key is valid and has access to Gemini models\\n\"\n",
        "            error_details += \"2. Verify you're using the latest google-generativeai package: pip install -U google-generativeai\\n\"\n",
        "            error_details += \"3. Check Google AI Studio (https://makersuite.google.com/) to verify available models\\n\"\n",
        "            return error_details\n",
        "\n",
        "# Step 4: Main workflow to tie everything together\n",
        "\n",
        "class GitHubErrorResolver:\n",
        "    def __init__(self, gemini_api_key: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the GitHub error resolver with necessary components.\n",
        "\n",
        "        Args:\n",
        "            gemini_api_key (str): Your Gemini API key for error analysis.\n",
        "        \"\"\"\n",
        "        self.temp_dir = tempfile.mkdtemp()\n",
        "        self.embedding_system = CodeEmbeddingSystem()\n",
        "        self.error_analyzer = ErrorAnalysisSystem(api_key=gemini_api_key)\n",
        "\n",
        "    def process_repository(self, repo_url: str) -> None:\n",
        "        \"\"\"Process a GitHub repository: clone, extract code, and store in vector DB.\"\"\"\n",
        "        print(f\"Processing repository: {repo_url}\")\n",
        "\n",
        "        # Clone the repository\n",
        "        repo_path = clone_github_repo(repo_url, self.temp_dir)\n",
        "\n",
        "        # Extract content from files\n",
        "        file_contents = get_file_contents(repo_path)\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = split_code_into_chunks(file_contents)\n",
        "\n",
        "        # Add to vector DB\n",
        "        self.embedding_system.add_chunks_to_db(chunks)\n",
        "\n",
        "        print(f\"Repository {repo_url} processed and stored in vector database.\")\n",
        "\n",
        "    def resolve_error(self, error_message: str) -> str:\n",
        "        \"\"\"Resolve an error by finding relevant code and generating a fix.\"\"\"\n",
        "        print(f\"Resolving error: {error_message}\")\n",
        "\n",
        "        # Query for similar code\n",
        "        similar_code = self.embedding_system.query_similar_code(error_message)\n",
        "\n",
        "        if not similar_code:\n",
        "            return \"No relevant code found for this error.\"\n",
        "\n",
        "        # Generate fix\n",
        "        fix = self.error_analyzer.generate_fix(error_message, similar_code)\n",
        "\n",
        "        return fix\n",
        "\n",
        "# Example usage in Google Colab\n",
        "def run_example():\n",
        "    # Install the latest version of the Gemini SDK\n",
        "    print(\"Installing latest Gemini SDK...\")\n",
        "    !pip install -qU google-generativeai\n",
        "\n",
        "    # Get Gemini API key (you would provide your actual key)\n",
        "    gemini_api_key = input(\"Enter your Gemini API key: \")\n",
        "\n",
        "    # Initialize the resolver with the API key\n",
        "    resolver = GitHubErrorResolver(gemini_api_key=gemini_api_key)\n",
        "\n",
        "    # Check available models before proceeding\n",
        "    try:\n",
        "        import google.generativeai as genai\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        print(\"\\nAvailable Gemini models:\")\n",
        "        for m in genai.list_models():\n",
        "            if \"gemini\" in m.name.lower():\n",
        "                print(f\" - {m.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing models: {e}\")\n",
        "\n",
        "    # Process a repository\n",
        "    repo_url = input(\"Enter GitHub repository URL (or press Enter for default example): \")\n",
        "    if not repo_url:\n",
        "        repo_url = \"https://github.com/sizzcode/build_test.git\"\n",
        "        print(f\"Using example repository: {repo_url}\")\n",
        "\n",
        "    resolver.process_repository(repo_url)\n",
        "\n",
        "    # Resolve an example error\n",
        "    error_message = \"\"\"\n",
        " File \"C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\Testing_error\\hello_world.py\", line 6\n",
        "    return \"h\n",
        "           ^\n",
        "SyntaxError: unterminated string literal (detected at line 6)\n",
        "    \"\"\"\n",
        "\n",
        "    # Allow custom error input\n",
        "    custom_error = input(\"Enter your error message (or press Enter for example error): \")\n",
        "    if custom_error:\n",
        "        error_message = custom_error\n",
        "\n",
        "    fix = resolver.resolve_error(error_message)\n",
        "    print(\"\\n\\nGenerated Fix:\")\n",
        "    print(fix)\n",
        "\n",
        "# Run a complete example in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    run_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWtPSTjm_hyk",
        "outputId": "10b78945-34bd-417c-849b-86d9171f3cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing latest Gemini SDK...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ougYj2mAPpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}